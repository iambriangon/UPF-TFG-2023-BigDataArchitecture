version: "3.9"

x-common:
  &common
  build:
    context: ./airflow
  image: airflow
  env_file:
    - ./airflow/.env
  user: "${AIRFLOW_UID}:0"
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - /var/run/docker.sock:/var/run/docker.sock

x-depends-on:
  &depends-on
  depends_on:
    postgres:
      condition: service_healthy
    airflow-init:
      condition: service_completed_successfully

services:
  namenode:
    build:
      context: ./hadoop/hadoop-namenode
    image: namenode
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://namenode:9870/"]
      interval: 10s
      timeout: 5s
      retries: 3
    ports:
      - '9870:9870'
    container_name: namenode
    
  datanode:
    build:
      context: ./hadoop/hadoop-datanode
    image: datanode
    depends_on:
      - namenode
    container_name: datanode

  hdfs-init:
    build:
      context: ./hadoop/hadoop-init
    image: hdfs-init
    depends_on:
      namenode:
        condition: service_healthy
    container_name: hdfs-init
  
  spark-master:
    build:
      context: ./spark/spark-master
    image: spark-master
    container_name: spark-master
    ports:
      - '8081:8080'

  spark-worker:
    build:
      context: ./spark/spark-worker
    image: spark-worker
    depends_on:
      - spark-master
    container_name: spark-worker

  hive-server:
    build:
      context: ./hive/hive-server
    image: hive-server
    depends_on:
      - metastore
    ports:
      - '10002:10002'
    container_name: hive-server

  metastore:
    build:
      context: ./hive/hive-metastore
    image: hive-metastore
    depends_on:
      hdfs-init:
        condition: service_completed_successfully
    container_name: hive-metastore

  watcher:
    build:
      context: ./watcher
    image: watcher
    depends_on:
      hdfs-init:
        condition: service_completed_successfully
    container_name: watcher
    environment:
      - PYTHONUNBUFFERED=1
    volumes:
      - ./watcher/files:/opt/watcher/files
      - ./watcher/pipeline.py:/opt/watcher/pipeline.py
      - ./watcher/upload:/opt/watcher/upload

  visualization:
    build:
      context: ./streamlit
    image: streamlit
    volumes:
      - ./streamlit/visualization.py:/app/visualization.py
    ports:
      - "8501:8501"
    container_name: streamlit
    tty: true

  postgres:
    image: postgres:13
    container_name: postgres
    ports:
      - "5434:5432"
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    env_file:
      - ./airflow/.env

  scheduler:
    <<: *common
    <<: *depends-on
    container_name: airflow-scheduler
    command: scheduler
    restart: on-failure
    ports:
      - "8793:8793"

  webserver:
    <<: *common
    <<: *depends-on
    container_name: airflow-webserver
    restart: always
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 30s
      retries: 5

  airflow-init:
    <<: *common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version

networks:
  default:
     name: newnetwork